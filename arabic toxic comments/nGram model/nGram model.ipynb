{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:23:57.112325Z","iopub.status.busy":"2022-05-28T14:23:57.111952Z","iopub.status.idle":"2022-05-28T14:24:04.095873Z","shell.execute_reply":"2022-05-28T14:24:04.095020Z","shell.execute_reply.started":"2022-05-28T14:23:57.112295Z"},"trusted":true},"outputs":[],"source":["import pandas as pd \n","import numpy as np\n","import re\n","import os\n","import argparse\n","import nltk\n","import string\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential,Model\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import SimpleRNN , Dropout ,LSTM,Conv1D, GlobalMaxPooling1D, MaxPooling1D , SpatialDropout1D,Activation,Dense, Dropout, Activation, Flatten, Input, concatenate , GRU\n","from keras.layers.embeddings import Embedding\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:13.204455Z","iopub.status.busy":"2022-05-28T14:24:13.203553Z","iopub.status.idle":"2022-05-28T14:24:13.442910Z","shell.execute_reply":"2022-05-28T14:24:13.441943Z","shell.execute_reply.started":"2022-05-28T14:24:13.204421Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["nltk.download('stopwords')\n","nltk.download('punkt')\n","stop_words = stopwords.words('arabic')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:15.053369Z","iopub.status.busy":"2022-05-28T14:24:15.052722Z","iopub.status.idle":"2022-05-28T14:24:25.120296Z","shell.execute_reply":"2022-05-28T14:24:25.119479Z","shell.execute_reply.started":"2022-05-28T14:24:15.053335Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(32377, 2)\n"]}],"source":["dataset1 = pd.read_csv('../input/arab-data/dataarabTOx.csv')\n","dataset6 = pd.read_csv('../input/arab-data/Book1.csv')\n","new = dataset6.dropna()\n","# print(new.iloc[10])\n","# print(new.iloc[9])\n","dataset7 = pd.read_csv('../input/arab-data/Book2.csv')\n","dataset = pd.read_csv('../input/data-set/toxic arabic tweets classification - Copy.tsv' , delimiter = '\\t' , quoting = 3)\n","dataset2 = pd.read_csv('../input/arabic-data/OSACT2020-sharedTask-dev.tsv' , delimiter = '\\t' , quoting = 3)\n","dataset3 = pd.read_csv('../input/arabic-data/OSACT2020-sharedTask-train.tsv' , delimiter = '\\t' , quoting = 3)\n","dataset4 =dataset2.iloc[:,0:2]\n","dataset5 =dataset3.iloc[:,0:2]\n","df_concat = pd.concat([dataset5,dataset,dataset4,new,dataset7,dataset1])\n","for i in range(0,len(df_concat)):\n","    if df_concat['Class'].iloc[i] == 'normal' or df_concat['Class'].iloc[i] == 'NOT_OFF' or df_concat['Class'].iloc[i] == 'Non-Offensive' or df_concat['Class'].iloc[i] == 0:\n","        df_concat['Class'].iloc[i] = 'N'\n","    elif df_concat['Class'].iloc[i] == 'abusive' or df_concat['Class'].iloc[i] == 'hate' or df_concat['Class'].iloc[i] == 'OFF' or df_concat['Class'].iloc[i] == 'Offensive' or df_concat['Class'].iloc[i] == -1 or df_concat['Class'].iloc[i] == -2:\n","        df_concat['Class'].iloc[i] = 'P'\n","print(df_concat.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:25.123481Z","iopub.status.busy":"2022-05-28T14:24:25.121780Z","iopub.status.idle":"2022-05-28T14:24:25.129499Z","shell.execute_reply":"2022-05-28T14:24:25.128762Z","shell.execute_reply.started":"2022-05-28T14:24:25.123442Z"},"trusted":true},"outputs":[],"source":["X = df_concat.iloc[: , 0].values\n","y = df_concat.iloc[: , -1].values\n","from keras.callbacks import ModelCheckpoint ,ReduceLROnPlateau ,EarlyStopping\n","\n","early = EarlyStopping(monitor='val_loss', mode='min', patience=4) \n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\n","callbacks_list = [early, learning_rate_reduction]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:25.131401Z","iopub.status.busy":"2022-05-28T14:24:25.130592Z","iopub.status.idle":"2022-05-28T14:24:25.175587Z","shell.execute_reply":"2022-05-28T14:24:25.174881Z","shell.execute_reply.started":"2022-05-28T14:24:25.131365Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["labelencoder_y=LabelEncoder()\n","y = labelencoder_y.fit_transform(y)\n","for i in range(0,len(y)):\n","    if y[i]==0 or y[i]==1:\n","        continue\n","    else:\n","        y[i]=0\n","print(max(y))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:25.177275Z","iopub.status.busy":"2022-05-28T14:24:25.176999Z","iopub.status.idle":"2022-05-28T14:24:25.186465Z","shell.execute_reply":"2022-05-28T14:24:25.185644Z","shell.execute_reply.started":"2022-05-28T14:24:25.177250Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 42)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:27.052511Z","iopub.status.busy":"2022-05-28T14:24:27.051251Z","iopub.status.idle":"2022-05-28T14:24:27.151901Z","shell.execute_reply":"2022-05-28T14:24:27.151096Z","shell.execute_reply.started":"2022-05-28T14:24:27.052463Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n","english_punctuations = string.punctuation\n","punctuations_list = arabic_punctuations + english_punctuations\n","\n","arabic_diacritics = re.compile(\"\"\"\n","                             ّ    | # Tashdid\n","                             َ    | # Fatha\n","                             ً    | # Tanwin Fath\n","                             ُ    | # Damma\n","                             ٌ    | # Tanwin Damm\n","                             ِ    | # Kasra\n","                             ٍ    | # Tanwin Kasr\n","                             ْ    | # Sukun\n","                             ـ     # Tatwil/Kashida\n","                         \"\"\", re.VERBOSE)\n","\n","\n","def normalize_arabic(text):\n","    text = re.sub(\"[إأآا]\", \"ا\", text)\n","    text = re.sub(\"ى\", \"ي\", text)\n","    text = re.sub(\"ؤ\", \"ء\", text)\n","    text = re.sub(\"ئ\", \"ء\", text)\n","    text = re.sub(\"ة\", \"ه\", text)\n","    text = re.sub(\"گ\", \"ك\", text)\n","    return text\n","\n","\n","def remove_diacritics(text):\n","    text = re.sub(arabic_diacritics, '', text)\n","    return text\n","\n","\n","def remove_punctuations(text):\n","    translator = str.maketrans('', '', punctuations_list)\n","    return text.translate(translator)\n","\n","\n","def remove_repeating_char(text):\n","    return re.sub(r'(.)\\1+', r'\\1', text)\n","\n","def remove_emojis(data):\n","    emoj = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002500-\\U00002BEF\"  # chinese char\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u\"\\U00010000-\\U0010ffff\"\n","        u\"\\u2640-\\u2642\" \n","        u\"\\u2600-\\u2B55\"\n","        u\"\\u200d\"\n","        u\"\\u23cf\"\n","        u\"\\u23e9\"\n","        u\"\\u231a\"\n","        u\"\\ufe0f\"  # dingbats\n","        u\"\\u3030\"\n","        u\"\\u2066\"\n","        u\"\\u2069\"\n","                      \"]+\", re.UNICODE)\n","    return re.sub(emoj, '', str(data))\n","\n","def to_lowercase(text):\n","    return text.lower()\n","\n","def remove_eng(text):\n","    return re.sub(r'[A-Za-z]','',str(text))\n","    \n","import string\n","def remove_punctuation(text):\n","    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n","    translator = str.maketrans('', '', string.punctuation)\n","    return text.translate(translator)\n","\n","\n","def replace_numbers(text):\n","    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n","    return re.sub(r'\\d+', '', text)\n","\n","\n","def remove_whitespaces(text):\n","    return text.strip()\n","\n","\n","def remove_stopwords(words, stop_words):\n","\n","    return [word for word in words if word not in stop_words]\n","\n","\n","def stem_words(words):\n","    \"\"\"Stem words in text\"\"\"\n","    stemmer = PorterStemmer()\n","    return [stemmer.stem(word) for word in words]\n","\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","def lemmatize_words(words):\n","    \"\"\"Lemmatize words in text\"\"\"\n","\n","    lemmatizer = WordNetLemmatizer()\n","    return [lemmatizer.lemmatize(word) for word in words]\n","\n","def lemmatize_verbs(words):\n","    \"\"\"Lemmatize verbs in text\"\"\"\n","\n","    lemmatizer = WordNetLemmatizer()\n","    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n","\n","def text2words(text):\n","  return word_tokenize(text)\n","\n","def normalize_text( text):\n","    text = remove_emojis(text)\n","    text = remove_eng(text)\n","    text = remove_punctuation(text)\n","    text = remove_punctuations(text)\n","    text = normalize_arabic(text)\n","    text = remove_diacritics(text)\n","    text = remove_repeating_char(text)\n","    text = to_lowercase(text)\n","    text = replace_numbers(text)\n","    words = text2words(text)\n","    words = remove_stopwords(words, stop_words)\n","    #words = stem_words(words)# Either stem ovocar lemmatize\n","    words = lemmatize_words(words)\n","    words = lemmatize_verbs(words)\n","\n","    return ''.join(words)\n","\n","def normalize_corpus(corpus):\n","  return [normalize_text(t) for t in corpus]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:29.363882Z","iopub.status.busy":"2022-05-28T14:24:29.363539Z","iopub.status.idle":"2022-05-28T14:24:45.926749Z","shell.execute_reply":"2022-05-28T14:24:45.925961Z","shell.execute_reply.started":"2022-05-28T14:24:29.363856Z"},"trusted":true},"outputs":[],"source":["X_train = normalize_corpus(X_train)\n","X_test = normalize_corpus(X_test)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:45.928719Z","iopub.status.busy":"2022-05-28T14:24:45.928371Z","iopub.status.idle":"2022-05-28T14:24:47.414297Z","shell.execute_reply":"2022-05-28T14:24:47.413457Z","shell.execute_reply.started":"2022-05-28T14:24:45.928684Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[  110  2515  9214 ...     0     0     0]\n"," [18845     0     0 ...     0     0     0]\n"," [    6  5006     0 ...     0     0     0]\n"," ...\n"," [28788 69726  1598 ...     0     0     0]\n"," [  151  1273   445 ...     0     0     0]\n"," [    3  3397    33 ...     0     0     0]]\n","(25901, 300)\n"]}],"source":["tok = Tokenizer()\n","tok.fit_on_texts(X_train + X_test)\n","vocab_size = len(tok.word_index) + 1\n","encoded_docs_train = tok.texts_to_sequences(X_train)\n","encoded_docs_test = tok.texts_to_sequences(X_test)\n","#encoded_docs_check = tok.texts_to_sequences(nor_check)\n","\n","max_length = 300\n","padded_docs_train = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')\n","padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n","#padded_docs_check = pad_sequences(encoded_docs_check, maxlen=max_length, padding='post')\n","print(padded_docs_train)\n","print(np.shape(padded_docs_train))\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:24:56.953042Z","iopub.status.busy":"2022-05-28T14:24:56.952396Z","iopub.status.idle":"2022-05-28T14:25:00.060463Z","shell.execute_reply":"2022-05-28T14:25:00.057743Z","shell.execute_reply.started":"2022-05-28T14:24:56.953005Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-05-28 14:24:57.072372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 300)]        0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 300)]        0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            [(None, 300)]        0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 300, 50)      3991950     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 300, 50)      3991950     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 300, 50)      3991950     input_3[0][0]                    \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 297, 32)      6432        embedding[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 295, 32)      9632        embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","conv1d_2 (Conv1D)               (None, 293, 32)      12832       embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 297, 32)      0           conv1d[0][0]                     \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 295, 32)      0           conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 293, 32)      0           conv1d_2[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling1d (MaxPooling1D)    (None, 148, 32)      0           dropout[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling1d_1 (MaxPooling1D)  (None, 147, 32)      0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling1d_2 (MaxPooling1D)  (None, 146, 32)      0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 4736)         0           max_pooling1d[0][0]              \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 4704)         0           max_pooling1d_1[0][0]            \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 4672)         0           max_pooling1d_2[0][0]            \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 14112)        0           flatten[0][0]                    \n","                                                                 flatten_1[0][0]                  \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 10)           141130      concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 1)            11          dense[0][0]                      \n","==================================================================================================\n","Total params: 12,145,887\n","Trainable params: 12,145,887\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["2022-05-28 14:24:57.231233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-28 14:24:57.232064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-28 14:24:57.233239: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-05-28 14:24:57.233566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-28 14:24:57.234274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-28 14:24:57.234885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-28 14:24:59.615540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-28 14:24:59.616533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-28 14:24:59.617289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-28 14:24:59.617878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]}],"source":["inputs1 = Input(shape=(max_length,))\n","embedding1 = Embedding(vocab_size, 50, input_length=max_length)(inputs1)\n","conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n","drop1 = Dropout(0.5)(conv1)\n","pool1 = MaxPooling1D(pool_size=2)(drop1)\n","flat1 = Flatten()(pool1)\n","# channel 2\n","inputs2 = Input(shape=(max_length,))\n","embedding2 = Embedding(vocab_size, 50, input_length=max_length)(inputs2)\n","conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n","drop2 = Dropout(0.5)(conv2)\n","pool2 = MaxPooling1D(pool_size=2)(drop2)\n","flat2 = Flatten()(pool2)\n","# channel 3\n","inputs3 = Input(shape=(max_length,))\n","embedding3 = Embedding(vocab_size, 50, input_length=max_length)(inputs3)\n","conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n","drop3 = Dropout(0.5)(conv3)\n","pool3 = MaxPooling1D(pool_size=2)(drop3)\n","flat3 = Flatten()(pool3)\n","# merge\n","merged = concatenate([flat1, flat2, flat3])\n","# interpretation\n","dense1 = Dense(10, activation='relu')(merged)\n","outputs = Dense(1, activation='sigmoid')(dense1)\n","model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n","# compile\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# summarize\n","model.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:25:05.570138Z","iopub.status.busy":"2022-05-28T14:25:05.569761Z","iopub.status.idle":"2022-05-28T14:25:53.240717Z","shell.execute_reply":"2022-05-28T14:25:53.239907Z","shell.execute_reply.started":"2022-05-28T14:25:05.570107Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-05-28 14:25:05.909846: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["2022-05-28 14:25:07.728335: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"]},{"name":"stdout","output_type":"stream","text":["648/648 [==============================] - 15s 10ms/step - loss: 0.5174 - accuracy: 0.7452 - val_loss: 0.3966 - val_accuracy: 0.8332\n","Epoch 2/10\n","648/648 [==============================] - 7s 10ms/step - loss: 0.1682 - accuracy: 0.9354 - val_loss: 0.3882 - val_accuracy: 0.8253\n","Epoch 3/10\n","648/648 [==============================] - 7s 10ms/step - loss: 0.0379 - accuracy: 0.9886 - val_loss: 0.4394 - val_accuracy: 0.8313\n","Epoch 4/10\n","648/648 [==============================] - 7s 10ms/step - loss: 0.0145 - accuracy: 0.9957 - val_loss: 0.5167 - val_accuracy: 0.8195\n","\n","Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n","Epoch 5/10\n","648/648 [==============================] - 7s 10ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 0.5497 - val_accuracy: 0.8340\n","Epoch 6/10\n","648/648 [==============================] - 6s 10ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.5752 - val_accuracy: 0.8342\n","\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n"]}],"source":["history = model.fit([padded_docs_train,padded_docs_train,padded_docs_train], y_train , validation_split = 0.2 , epochs=10 , batch_size = 32, callbacks=callbacks_list)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:26:00.808141Z","iopub.status.busy":"2022-05-28T14:26:00.807786Z","iopub.status.idle":"2022-05-28T14:26:29.420506Z","shell.execute_reply":"2022-05-28T14:26:29.419538Z","shell.execute_reply.started":"2022-05-28T14:26:00.808113Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["203/203 [==============================] - 1s 3ms/step - loss: 0.5768 - accuracy: 0.8355\n","Accuracy: 83.554661    0.5767640471458435\n"]}],"source":["loss, accuracy = model.evaluate([padded_docs_test,padded_docs_test,padded_docs_test], y_test)\n","print('Accuracy: %f' % (accuracy*100),'  ',loss)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T14:51:38.779314Z","iopub.status.busy":"2022-05-28T14:51:38.778501Z","iopub.status.idle":"2022-05-28T14:53:26.483050Z","shell.execute_reply":"2022-05-28T14:53:26.482136Z","shell.execute_reply.started":"2022-05-28T14:51:38.779277Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["User:  \n"]},{"name":"stdout","output_type":"stream","text":["[[0.32507703]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  kneklvn\n"]},{"name":"stdout","output_type":"stream","text":["[[0.32507703]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  الحمد لله\n"]},{"name":"stdout","output_type":"stream","text":["[[0.00722317]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  الله يرحمه ويثبته عند السؤال\n"]},{"name":"stdout","output_type":"stream","text":["[[0.0002771]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  اخص عليك واطى\n"]},{"name":"stdout","output_type":"stream","text":["[[0.9908756]]\n","abusive\n"]},{"name":"stdout","output_type":"stream","text":["User:  ياعم اقعد بقه\n"]},{"name":"stdout","output_type":"stream","text":["[[0.02211029]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  يسرمزط\n"]},{"name":"stdout","output_type":"stream","text":["[[0.32507703]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  يشرموط\n"]},{"name":"stdout","output_type":"stream","text":["[[0.26073435]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  كسمك\n"]},{"name":"stdout","output_type":"stream","text":["[[0.9976165]]\n","abusive\n"]},{"name":"stdout","output_type":"stream","text":["User:  بس يشرموط\n"]},{"name":"stdout","output_type":"stream","text":["[[0.26073435]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  شرموط\n"]},{"name":"stdout","output_type":"stream","text":["[[0.9916768]]\n","abusive\n"]},{"name":"stdout","output_type":"stream","text":["User:  انا تعبت منك\n"]},{"name":"stdout","output_type":"stream","text":["[[0.11133708]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  اخرس شويه\n"]},{"name":"stdout","output_type":"stream","text":["[[0.25727665]]\n","normal\n"]},{"name":"stdout","output_type":"stream","text":["User:  quit\n"]}],"source":["import keras\n","while True:\n","        \n","        inp = input(\"User: \")\n","        \n","        if inp.lower() == \"quit\":\n","            break\n","        else:\n","            z = keras.preprocessing.sequence.pad_sequences(tok.texts_to_sequences(normalize_corpus([inp])),maxlen=300, padding='post')\n","            #result1 = model1.predict([z,z,z])\n","            result = model.predict([z,z,z])\n","            #avg = (result1+result2)/2\n","            print(result)\n","            dis = np.argmax(result)\n","            if result <= 0.5:\n","                print(\"normal\")\n","            else:\n","                print(\"abusive\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
