{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f6fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import SimpleRNN , Dropout ,LSTM,Conv1D, GlobalMaxPooling1D, MaxPooling1D , SpatialDropout1D,Activation,Dense, Dropout, Activation, Flatten, Input, concatenate , GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb56e070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82e0fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\Book2.csv')\n",
    "dataset2 = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\OSACT2020-sharedTask-dev.tsv' , delimiter = '\\t' , quoting = 3)\n",
    "dataset3 = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\OSACT2020-sharedTask-train.tsv' , delimiter = '\\t' , quoting = 3)\n",
    "dataset4 =dataset2.iloc[:,0:2]\n",
    "dataset5 =dataset3.iloc[:,0:2]\n",
    "df_concat = pd.concat([dataset5,dataset4])\n",
    "for i in range(0,len(df_concat)):\n",
    "    if df_concat['Class'].iloc[i] == 'normal' or df_concat['Class'].iloc[i] == 'NOT_OFF' or df_concat['Class'].iloc[i] == 'Non-Offensive' or df_concat['Class'].iloc[i] == 0:\n",
    "        df_concat['Class'].iloc[i] = 'N'\n",
    "    elif df_concat['Class'].iloc[i] == 'abusive' or df_concat['Class'].iloc[i] == 'hate' or df_concat['Class'].iloc[i] == 'OFF' or df_concat['Class'].iloc[i] == 'Offensive' or df_concat['Class'].iloc[i] == -1 or df_concat['Class'].iloc[i] == -2:\n",
    "        df_concat['Class'].iloc[i] = 'P'\n",
    "print(df_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d49e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_concat.iloc[: , 0].values\n",
    "y = df_concat.iloc[: , -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5213bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "labelencoder_y=LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "for i in range(0,len(y)):\n",
    "    if y[i]==0 or y[i]==1:\n",
    "        continue\n",
    "    else:\n",
    "        y[i]=0\n",
    "print(max(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b347511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bee9050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        u\"\\u2066\"\n",
    "        u\"\\u2069\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', str(data))\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_eng(text):\n",
    "    return re.sub(r'[A-Za-z]','',str(text))\n",
    "    \n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def replace_numbers(text):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_stopwords(words, stop_words):\n",
    "\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in text\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize words in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
    "\n",
    "def text2words(text):\n",
    "  return word_tokenize(text)\n",
    "\n",
    "def normalize_text( text):\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_eng(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = remove_diacritics(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = replace_numbers(text)\n",
    "    words = text2words(text)\n",
    "    words = remove_stopwords(words, stop_words)\n",
    "    #words = stem_words(words)# Either stem ovocar lemmatize\n",
    "    words = lemmatize_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "\n",
    "    return ''.join(words)\n",
    "\n",
    "def normalize_corpus(corpus):\n",
    "  return [normalize_text(t) for t in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "664e472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize_corpus(X_train)\n",
    "X_test = normalize_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36833ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   78  6418   222 ...     0     0     0]\n",
      " [  117   901 10980 ...     0     0     0]\n",
      " [ 2796  1952   769 ...     0     0     0]\n",
      " ...\n",
      " [28485 28486  1806 ...     0     0     0]\n",
      " [  656   460  2802 ...     0     0     0]\n",
      " [    5   406   882 ...     0     0     0]]\n",
      "(6400, 300)\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(X_train + X_test)\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "encoded_docs_train = tok.texts_to_sequences(X_train)\n",
    "encoded_docs_test = tok.texts_to_sequences(X_test)\n",
    "#encoded_docs_check = tok.texts_to_sequences(nor_check)\n",
    "\n",
    "max_length = 300\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
    "#padded_docs_check = pad_sequences(encoded_docs_check, maxlen=max_length, padding='post')\n",
    "print(padded_docs_train)\n",
    "print(np.shape(padded_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ecd2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100 , dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73dd4fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4dc1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 300, 100)          3290100   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 300, 100)          0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,370,601\n",
      "Trainable params: 3,370,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0112ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 51s 10s/step - loss: 0.6372 - accuracy: 0.8029 - val_loss: 0.5712 - val_accuracy: 0.7812\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 48s 10s/step - loss: 0.5198 - accuracy: 0.8094 - val_loss: 0.5737 - val_accuracy: 0.7812\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 48s 10s/step - loss: 0.4939 - accuracy: 0.8094 - val_loss: 0.5259 - val_accuracy: 0.7812\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 49s 10s/step - loss: 0.4942 - accuracy: 0.8094 - val_loss: 0.5259 - val_accuracy: 0.7812\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 48s 10s/step - loss: 0.4895 - accuracy: 0.8094 - val_loss: 0.5297 - val_accuracy: 0.7812\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 57s 12s/step - loss: 0.4880 - accuracy: 0.8094 - val_loss: 0.5342 - val_accuracy: 0.7812\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 53s 10s/step - loss: 0.4887 - accuracy: 0.8094 - val_loss: 0.5277 - val_accuracy: 0.7812\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 53s 11s/step - loss: 0.4875 - accuracy: 0.8094 - val_loss: 0.5260 - val_accuracy: 0.7812\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 56s 11s/step - loss: 0.4881 - accuracy: 0.8094 - val_loss: 0.5269 - val_accuracy: 0.7812\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 59s 12s/step - loss: 0.4872 - accuracy: 0.8094 - val_loss: 0.5292 - val_accuracy: 0.7812\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_docs_train, y_train , validation_split = 0.2 , epochs=10 , batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591ade9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 2s 45ms/step - loss: 0.5125 - accuracy: 0.7919\n",
      "Accuracy: 79.187500    0.5124989151954651\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs_test, y_test)\n",
    "print('Accuracy: %f' % (accuracy*100),'  ',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "797a607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"try_model_LSTM.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e870ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# to save the fitted tokenizer\n",
    "with open('tokenizer_LSTM.pickle', 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted label encoder\n",
    "with open('label_encoder_y_LSTM.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(labelencoder_y, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
