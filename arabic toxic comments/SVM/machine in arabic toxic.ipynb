{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c0cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import SimpleRNN , Dropout ,LSTM,Conv1D, GlobalMaxPooling1D, MaxPooling1D , SpatialDropout1D,Activation,Dense, Dropout, Activation, Flatten, Input, concatenate , GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f514e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a4a494f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15998, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\dataarabTOx.csv')\n",
    "dataset6 = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\Book1.csv')\n",
    "new = dataset6.dropna()\n",
    "# print(new.iloc[10])\n",
    "# print(new.iloc[9])\n",
    "dataset7 = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\Book2.csv')\n",
    "dataset = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\toxic arabic tweets classification - Copy.tsv' , delimiter = '\\t' , quoting = 3)\n",
    "dataset2 = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\OSACT2020-sharedTask-dev.tsv' , delimiter = '\\t' , quoting = 3)\n",
    "dataset3 = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\OSACT2020-sharedTask-train.tsv' , delimiter = '\\t' , quoting = 3)\n",
    "dataset4 =dataset2.iloc[:,0:2]\n",
    "dataset5 =dataset3.iloc[:,0:2]\n",
    "df_concat = pd.concat([dataset,dataset5,dataset4])\n",
    "for i in range(0,len(df_concat)):\n",
    "    if df_concat['Class'].iloc[i] == 'normal' or df_concat['Class'].iloc[i] == 'NOT_OFF' or df_concat['Class'].iloc[i] == 'Non-Offensive' or df_concat['Class'].iloc[i] == 0:\n",
    "        df_concat['Class'].iloc[i] = 'N'\n",
    "    elif df_concat['Class'].iloc[i] == 'abusive' or df_concat['Class'].iloc[i] == 'hate' or df_concat['Class'].iloc[i] == 'OFF' or df_concat['Class'].iloc[i] == 'Offensive' or df_concat['Class'].iloc[i] == -1 or df_concat['Class'].iloc[i] == -2:\n",
    "        df_concat['Class'].iloc[i] = 'P'\n",
    "print(df_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f9c09f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7364, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('C:\\\\Users\\\\Mahmoud\\\\Desktop\\\\toxic arabic tweets classification.tsv' , delimiter = '\\t' , quoting = 3)\n",
    "df = pd.concat([dataset])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5ab796e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الوزير جبران باسيل تاج راسك يا جربان ممنوع بعد...</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>صديقي انت ابن جامعه اللعبه اكبر من داعش اللعبه...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>و مصلحة لبنان تبدأ باستخراج النفط و الغاز لوقف...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كسمك يبن الشرموطه انت و الشرموطة امك يبن الاحبة</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>كسمين امك يبن كوم الزواني انت و الى جابك</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>يا جبران باسيل يا معلم يا ريس يا استاذ بدك حضن...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ما سمعنا جعاركم مبارح على القصف الإسرائيلي يلي...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>نصيحة احكي مع الرئيس ميشال عون او هاجم جبران ب...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>يغبي يبن الغبى يحرق كسمين امك</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>يبن الشرموطة يبن الشرموطه يحرق كسمينك</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet    Class\n",
       "0  الوزير جبران باسيل تاج راسك يا جربان ممنوع بعد...  abusive\n",
       "1  صديقي انت ابن جامعه اللعبه اكبر من داعش اللعبه...   normal\n",
       "2  و مصلحة لبنان تبدأ باستخراج النفط و الغاز لوقف...   normal\n",
       "3   كسمك يبن الشرموطه انت و الشرموطة امك يبن الاحبة   abusive\n",
       "4       كسمين امك يبن كوم الزواني انت و الى جابك      abusive\n",
       "5  يا جبران باسيل يا معلم يا ريس يا استاذ بدك حضن...   normal\n",
       "6  ما سمعنا جعاركم مبارح على القصف الإسرائيلي يلي...     hate\n",
       "7  نصيحة احكي مع الرئيس ميشال عون او هاجم جبران ب...   normal\n",
       "8                      يغبي يبن الغبى يحرق كسمين امك  abusive\n",
       "9             يبن الشرموطة يبن الشرموطه يحرق كسمينك   abusive"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "590d8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_concat.iloc[: , 0].values\n",
    "y = df_concat.iloc[: , -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e0dfd197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10389   5609\n"
     ]
    }
   ],
   "source": [
    "labelencoder_y=LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "dataset[\"label\"] = labelencoder_y.fit_transform(dataset[\"Class\"])\n",
    "for i in range(0,len(y)):\n",
    "    if y[i]==0 or y[i]==1:\n",
    "        continue\n",
    "    else:\n",
    "        y[i]=0\n",
    "print(max(y))\n",
    "z = 0\n",
    "p =0\n",
    "for i in range(0,len(y)):\n",
    "    if y[i]==0:\n",
    "        z=z+1\n",
    "    elif y[i]==1:\n",
    "        p=p+1\n",
    "print(z,' ',p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7ebfcf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0f8617a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        u\"\\u2066\"\n",
    "        u\"\\u2069\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', str(data))\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_eng(text):\n",
    "    return re.sub(r'[A-Za-z]','',str(text))\n",
    "    \n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def replace_numbers(text):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_stopwords(words, stop_words):\n",
    "\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in text\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize words in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
    "\n",
    "def text2words(text):\n",
    "  return word_tokenize(text)\n",
    "\n",
    "def normalize_text( text):\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_eng(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = remove_diacritics(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = replace_numbers(text)\n",
    "    words = text2words(text)\n",
    "    words = remove_stopwords(words, stop_words)\n",
    "    #words = stem_words(words)# Either stem ovocar lemmatize\n",
    "    words = lemmatize_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "\n",
    "    return ''.join(words)\n",
    "\n",
    "def normalize_corpus(corpus):\n",
    "  return [normalize_text(t) for t in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7914f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize_corpus(X_train)\n",
    "X_test = normalize_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "72c10ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1,3), tokenizer=text2words,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "107a8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vec.fit_transform(X_train)\n",
    "X_test = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bba4ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.28125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "classifier1 = SVC(kernel = 'rbf',C = 10000,probability=False)\n",
    "classifier1.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = classifier1.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred1)\n",
    "print(score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "537a65f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2008,   80],\n",
       "       [ 327,  785]], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test , y_pred1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "58bd762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hggi \n",
      "[0]\n",
      "normal\n",
      "User: \n",
      "[0]\n",
      "normal\n",
      "User: الله اكبر\n",
      "[0]\n",
      "normal\n",
      "User: الله\n",
      "[0]\n",
      "normal\n",
      "User: الحمد لله\n",
      "[0]\n",
      "normal\n",
      "User: يسكسمك\n",
      "[0]\n",
      "normal\n",
      "User: كسمك\n",
      "[1]\n",
      "abusive\n",
      "User: انت بتقول ايه؟؟\n",
      "[0]\n",
      "normal\n",
      "User: بس يلا\n",
      "[0]\n",
      "normal\n",
      "User: يشرموط\n",
      "[1]\n",
      "abusive\n",
      "User: الله يرحمه ويسكنه فسيح جناته\n",
      "[0]\n",
      "normal\n",
      "User: يارب\n",
      "[0]\n",
      "normal\n",
      "User: رب\n",
      "[0]\n",
      "normal\n",
      "User: الحمدلله ع كل شئ\n",
      "[0]\n",
      "normal\n",
      "User: ضعه\n",
      "[0]\n",
      "normal\n",
      "User: quit\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "while True:\n",
    "        \n",
    "        inp = input(\"User: \")\n",
    "        \n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "        else:\n",
    "            result = classifier1.predict(vec.transform(normalize_corpus([inp])))\n",
    "            print(result)\n",
    "            if result == 0:\n",
    "                print(\"normal\")\n",
    "            else:\n",
    "                print(\"abusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "af3b746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# to save the fitted model\n",
    "with open('SVM_model0.pickle', 'wb') as model:\n",
    "    pickle.dump(classifier1, model, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted tokenizer\n",
    "with open('vec0.pickle', 'wb') as ve:\n",
    "    pickle.dump(vec, ve, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb741cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(oov_token='UNK')\n",
    "tok.fit_on_texts(X_train + X_test)\n",
    "X_train = tok.texts_to_matrix(X_train , mode='tfidf')\n",
    "X_test = tok.texts_to_matrix(X_test, mode='tfidf')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sm = StandardScaler()\n",
    "X_train = sm.fit_transform(X_train)\n",
    "X_test = sm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37c6bfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.1875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "classifier1 = SVC(kernel = 'rbf',C = 10000)\n",
    "classifier1.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = classifier1.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred1)\n",
    "print(score*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
